# Triton Deployment - SAM3 ONNX

## ğŸ“Œ Overview
Deploy SAM3 ONNX model using NVIDIA Triton Inference Server optimized for RTX 3090 (24GB VRAM).

This repository contains:

* Exported SAM3 ONNX model
* Triton model repository structure
* Docker deployment setup

---

## ğŸ§  Model Info

* Model: SAM3 (ONNX)
* Framework: ONNXRuntime
* Inference Server: Triton 24.08+
* GPU: NVIDIA (CUDA required)

âš ï¸ Note: If model > 2GB, ONNX external data format must be used.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ build
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ repo
â”‚   â”œâ”€â”€ sam3_decoder
â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ config.pbtxt
â”‚   â”œâ”€â”€ sam3_image_encoder
â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ config.pbtxt
â”‚   â”œâ”€â”€ sam3_language_encoder
â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ config.pbtxt
â”‚   â””â”€â”€ sam3_pipeline
â”‚       â”œâ”€â”€ 1
â”‚       â””â”€â”€ config.pbtxt
â””â”€â”€ source
    â””â”€â”€ sam3-onnx
        â”œâ”€â”€ assets
        â”œâ”€â”€ check.py
        â”œâ”€â”€ export_onnx.py
        â”œâ”€â”€ images
        â”œâ”€â”€ infer_onnx.py
        â”œâ”€â”€ infer_torch.py
        â”œâ”€â”€ infer_triton.py
        â”œâ”€â”€ LICENSE
        â”œâ”€â”€ Makefile
        â”œâ”€â”€ models
        â”œâ”€â”€ pyproject.toml
        â”œâ”€â”€ README.md
        â”œâ”€â”€ sam3
        â””â”€â”€ uv.lock
```

---

## ğŸš€ Run Triton Server

```bash
docker compose up --build 
```

---

## ğŸ” Check Model Status


```
http://localhost:8000/v2/health/ready
```


---

## ğŸ§ª Test Inference (Python Client)

Use the provided Triton client script:

ğŸ‘‰ *[Run infer_triton.py](source/sam3-onnx/infer_triton.py)*

Example:

```bash
cd source/sam3-onnx
uv infer_triton.py \
    --image images/bus.jpg \
    --text-prompt "person"

